{"name":"Tiny TinyML","tagline":"","body":"## Tiny TinyML\r\nAn exploration of running Machine Learning algorithms on really, really small computers.\r\n\r\n![](https://cdn.dribbble.com/users/2424774/screenshots/16962879/media/d0bd0d738aa6d19fc94bcd06866abe3c.png)\r\n\r\n### What is Machine Learning?\r\nThe word Machine Learning has certainly captured the imaginations of Computer Science majors and corporate boardrooms alike. It feels like magic - you take a handful of numbers, throw them at a neural network, and suddenly the computer can play chess or identify a flower or do your job better than you. However, it's not as mystical as it seems; the machine learning problem is something we've been doing since middle school Algebra - finding the maximum or minimum of a graph.\r\n\r\nTo learn how to solve a problem, the computer first needs to understand what the problem is. We define these problems in terms of Loss functions - a function that tests out our model and tells us how well (or how poorly) we are doing. For example, if we were trying to define a Loss function for the [Identifying Flowers] problem, we might choose to give our model one point when it correctly tells us what flower we're looking at and zero points otherwise. Then, we try to find some maximum (or minimum) on the Loss function by tweaking the parameters of our model - using the gradient of the Loss function to find the quickest way toward the optimal point we're trying to find. \r\n\r\n### What About Neural Networks?\r\nIt's hard to blame people for liking Neural Networks, what with their strange hidden layers and esoteric activation functions. Largely treated like black boxes, they're remarkably flexible, able to take in almost any dataset and draw some conclusion from them. Many other machine learning algorithms either have stringent requirements for the shape of the dataset - i.e. being in a straight line, being separated by a straight line, etc. - or we are required to know how to transform the data to fit the patterns that the algorithm finds acceptable. The former is often rare in life, while the latter is often hard and requires time and effort. However, the neural network suffers neither of these concerns - which is why they're used in almost every machine learning application to this day.\r\n\r\nThe neural network achieves such flexibility by being more modular than its linear friends. Think of a neural net like a LEGO Death Star. The individual LEGO blocks don't look like much. Some are long, some are short, some are flat, some are slanted. They're great at modelling rectangular things - the back of an eighteen wheeler, a brick, a train if you try hard enough - but they just don't have the complexity to model the organic, free-flowing shapes we see in the natural world. However, if we combine a bunch of LEGO blocks in the right way, after a lot of time and effort, we've built something a lot cooler. In the case of a neural net, it models a bunch of tiny linear models that can be turned on or off at certain points. As you see in the picture below, as you add more and more linear models, the model we end up with - that the neural net learns - looks an awful lot like the ground truth.\r\n\r\n<Add Pic Here>\r\n\r\n### That's Great And All, But What Does It Have To Do With Small Computers?\r\nThe Achilles' Heel of the fanciest of Machine Learning models is how expensive they are to train. We can teach a computer how to play poker - well enough to consistently beat the best human players out there, actually! - but the only people who have the compute needed to train these models are the research labs of the likes of Facebook and Google (or many of the research labs at Cornell). Even if the select few who have the computer power were kind enough to train models for the rest of us, they only have so many servers and so many hours in a day. A general purpose model trained by Google would be incredible at general purpose tasks, but most use cases for machine learning are not for general purpose tasks. Most of the time, we have one specific thing we want our model to be able to do, whether we want it to recognize our voice when we tell Alexa to buy a new bottle of dish soap or we want it to recognize our faces whether we're wearing glasses or not. In these cases, the models would be running on very small computers. What they learn would also need to be confined to the specific device alone - nobody wants any Joe Schmoe to have access to [The Dummy's Guide to Recognizing [Your Name]'s Face From A Ring Camera]. Hence, the rise of TinyML.\r\n\r\nThe problem of TinyML is twofold - loading a model on a tiny computer and training a model on a tiny computer. Both require stringent memory management - given the small device size, we must make sure that every byte is used as efficiently as we can. The former task is (from a computer power point of view) generally easier; we can make all the adjustments we need on a PC and give a completed model to the tiny computer. The latter task is more compute intensive - more intermediary math is required to calculate the various losses. This project [is currently aiming to] tackle the latter task, following [a paper published by Hanif Heidari and Andrei A. Velichko](https://arxiv.org/abs/2105.14412).\r\n\r\n### How Are You Doing This?\r\nI plan on using a similar Neural Net architecture on [either MNIST or the accel data, haven't decided yet]. The dataset will be split into chunks, since there isn't enough flash memory to house the whole thing. The model will be trained on these small chunks for a number of steps at a time. Once each 'chunk' is done, the model will be written to flash for storage while the old 'chunk' of training data is removed and a new 'chunk' of data is put into the device. Ideally, the model trained by this method will be able to perform with >50% accuracy.\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}